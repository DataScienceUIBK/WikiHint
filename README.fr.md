[English](README.md) | [ä¸­æ–‡](README.zh.md) | [Deutsch](README.de.md) | [FranÃ§ais](README.fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)

# WikiHint : Un Jeu de DonnÃ©es AnnotÃ© par des Humains pour le Classement et la GÃ©nÃ©ration d'Indices
<a href="https://huggingface.co/datasets/JamshidJDMY/WikiHint"><img alt="Huggingface" src="https://img.shields.io/static/v1?label=Dataset&message=WikiHint&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAIoUExURQAAAP/////57v/67xUVFf/clv+KAP/uzf/sxv8AAP9TAP/ltP///v/ouP/////////////////////////////8+v/pvP/biP/Vbf/Vbv/cif/qv//9+//////////03v/Yev/Zfv/14//25v/Uav/Vbv/46//////dkf/gmP/////04P/25//pvP/sxf/////lr//ouP/qwP/tyf/msv/ntf/+/P/36f/LUf/36P/w0v/w0//w0//w0//78//gm//QZv/RZv/gm//78v/////14v/nt//gnP/hn//w0f/////w0f/hn//gnP/nt//14v/////////////////LLv/MGv/PGf/LL//LG//THP/THv/SHv/UHv/LGv/LH/7SHuK8JOzDIuW+I+jAI//LHv/PTP/NF/PBG3BkOpyGMvrOH4R0NoV0NvzJGv/MF//QT//MLv/LGPu/FNayJu7FIdq2Jf7DFP/JF//ML//LJurCIsCiKujCIubAI7+hK/DHIf/LJ//HK//NGf/SHeS9IlxSP25QQmtOQmVZPu3DIf/RHf/HLP++Kf/AD/++EP3EFNCfLNhpQthrQdinKv/FFP/AEP+/K/++Dv/BEv+/Ef/CE//MIf/NIP/MGf++D//KTP/FOP/DE//PG//PHP/JGP/EFP/EM//BDf/TH//GFP/CEP/DEP/EEv/BDv/MS//IJ//JHf/JHP/JP//IQf/IHP/IJv/LSf///7SHOh0AAABUdFJOUwAAAAAAAAAAAAAAAAAABiZCQykIAUGn3/Hy4q5KAwRy7vJ/Yfb7cR/X4ipkdpepAqi5mavM2z5v/pGTtZS2QtP4999bIGyry8yUR4fJzbJ2BRIRE9ZoIHEAAAABYktHRAH/Ai3eAAAAB3RJTUUH6AIGEyohVAr+rAAAARZJREFUGNNjYGBgZGTi4xcQFBJmZmRkYQDxRUTFxCUkpaRlZBkZQXw5eYWQ0LCw0HBFJWGgCCOrskpEZFR0dExkrKoaG1BAXSMuPiExOjopOSpFU4uRgV07NS09IzMqKzsnNy9fh4OBU7egsKi4JCo6ubSsvEJPlkHfoDIqqqq6prauPiqqwVCYgcuosam5pbWtvaOzq6nbWJZBy6Snt69/wsRJk6f0TZ1masZgbjF9xsxZhbPnzJ01c8a8+ZYMVgt6F5aHLly0eGHokqVTl1kz2CxYXhi1YuWq1WuiouauXWbLYGe/bv2GjZscHDdv2bh1m5MzA7eLq5u7h6eXoLePr5+/ENDpjBwBgYH6PIyMQcF8vIyMAKnZUpQQgaV4AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDI0LTAyLTA2VDE5OjQyOjI1KzAwOjAwybP6HAAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyNC0wMi0wNlQxOTo0MjoyNSswMDowMLjuQqAAAAAodEVYdGRhdGU6dGltZXN0YW1wADIwMjQtMDItMDZUMTk6NDI6MzMrMDA6MDBAgVbbAAAAAElFTkSuQmCC&color=20BEFF"/></a>
<a href="https://doi.org/10.1145/3726302.3730284"><img src="https://img.shields.io/static/v1?label=Paper&message=ACM%20SIGIR&color=green&logo=arxiv"></a>
<a href="https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb"><img src="https://img.shields.io/static/v1?label=Colab&message=Demo&logo=Google%20Colab&color=f9ab00"></a>
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-blue)](https://creativecommons.org/licenses/by/4.0/)
<img src="https://github.com/DataScienceUIBK/WikiHint/blob/main/WikiHint/Pipeline.png">

WikiHint est un **jeu de donnÃ©es annotÃ© par des humains** conÃ§u pour la **gÃ©nÃ©ration et le classement automatiques d'indices** pour les questions factuelles. Ce jeu de donnÃ©es, basÃ© sur Wikipedia, contient **5 000 indices pour 1 000 questions** et soutient la recherche sur **lâ€™Ã©valuation, le classement et la gÃ©nÃ©ration dâ€™indices**.

## <img src="https://github.com/DataScienceUIBK/TriviaHG/blob/main/Framework/gif-dan.gif" width="32" height="32"/> Attention <img src="https://github.com/DataScienceUIBK/TriviaHG/blob/main/Framework/gif-dan.gif" width="32" height="32"/>

Ã€ partir de **fÃ©vrier 2025**, nous recommandons dâ€™utiliser **HintEval**, le framework pour la **gÃ©nÃ©ration et lâ€™Ã©valuation dâ€™indices**. HintEval inclut le **jeu de donnÃ©es WikiHint** ainsi que les mÃ©triques dâ€™Ã©valuation prÃ©sentÃ©es dans lâ€™article WikiHint, rendant le travail avec les indices plus simple que jamais.

DÃ©couvrez HintEval ici :  
- ğŸ“– **[Documentation HintEval](http://hinteval.readthedocs.io/)**  
- ğŸ“¦ **[Installation HintEval via PyPI](https://pypi.org/project/hinteval/)**  
- ğŸ’» **[DÃ©pÃ´t GitHub HintEval](https://github.com/DataScienceUIBK/HintEval)**  
- ğŸ“œ **[Article HintEval (arXiv)](https://doi.org/10.48550/arXiv.2502.00857)**  

Pour une **intÃ©gration fluide** de la gÃ©nÃ©ration et de lâ€™Ã©valuation dâ€™indices, nous vous recommandons vivement de **migrer** vers **HintEval** !

## ğŸ—‚ Vue dâ€™ensemble

- **1 000 questions** avec **5 000 indices crÃ©Ã©s manuellement**.
- Indices classÃ©s par des **annotateurs humains** selon leur utilitÃ©.
- Ã‰valuÃ© Ã  l'aide de **LLMs (LLaMA, GPT-4)** et dâ€™**Ã©tudes sur la performance humaine**.
- Supporte le **classement des indices** et **lâ€™Ã©valuation automatique des indices**.

## ğŸ”¬ Contributions Ã  la recherche

âœ… **Premier jeu de donnÃ©es annotÃ© par des humains** pour la gÃ©nÃ©ration et le classement d'indices.  
âœ… **HintRank** : Une mÃ©thode lÃ©gÃ¨re pour le classement automatique des indices.  
âœ… **Ã‰tude humaine** Ã©valuant lâ€™efficacitÃ© des indices pour aider les utilisateurs Ã  rÃ©pondre aux questions.  
âœ… **Ajustement fin des LLM open-source** (LLaMA-3.1, GPT-4) pour la gÃ©nÃ©ration dâ€™indices.  

## ğŸ“ˆ Principaux enseignements

- **Les indices tenant compte de la rÃ©ponse** amÃ©liorent leur efficacitÃ©.  
- **Les modÃ¨les LLaMA ajustÃ©s finement** gÃ©nÃ¨rent de meilleurs indices que les modÃ¨les bruts.  
- **Les indices plus courts** ont tendance Ã  Ãªtre **plus efficaces** que les plus longs.  
- **Les indices gÃ©nÃ©rÃ©s par des humains** surpassent ceux des LLM en termes de clartÃ© et de classement.

## ğŸš€ Prise en main

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t

```sh
git clone https://github.com/DataScienceUIBK/WikiHint.git
cd WikiHint
```

### 2ï¸âƒ£ Charger le jeu de donnÃ©es

```python
import json

with open("./WikiHint/training.json", "r") as f:
    training_data = json.load(f)

with open("./WikiHint/test.json", "r") as f:
    test_data = json.load(f)

print(f"Ensemble dâ€™entraÃ®nement : {len(training_data)} questions")
print(f"Ensemble de test : {len(test_data)} questions")
```

## ğŸ† HintRank : Une MÃ©thode LÃ©gÃ¨re de Classement des Indices

HintRank est une **mÃ©thode de classement automatique** des indices utilisant des **modÃ¨les basÃ©s sur BERT**. Il fonctionne sur le principe des **comparaisons par paires**, dÃ©terminant la **pertinence relative des indices**.

<p align="center">
    <img src="https://raw.githubusercontent.com/DataScienceUIBK/WikiHint/main/HintRank/EvaluationMethod.png" width="35%">
</p>

### âœ¨ FonctionnalitÃ©s :
âœ” **LÃ©ger** : Fonctionne localement sans nÃ©cessiter de ressources informatiques massives.  
âœ” **Ã‰valuation sans LLM** : Fonctionne sans dÃ©pendre de **grands modÃ¨les gÃ©nÃ©ratifs**.  
âœ” **Classement alignÃ© sur les humains** : Forte corrÃ©lation avec le **classement des indices Ã©tabli par des humains**.  

### ğŸ” Comment Ã§a marche ?
1. **ConcatÃ¨ne la question & deux indices** â†’ Les convertit au format compatible BERT.  
2. **Calcule la qualitÃ© des indices** â†’ DÃ©termine quel indice est **le plus utile**.  
3. **GÃ©nÃ¨re un classement des indices** â†’ Attribue un rang basÃ© sur des comparaisons par paires.  

## ğŸ“Œ Utilisation de `HintRank` pour le Classement des Indices

Le module `HintRank` est conÃ§u pour **classer automatiquement les indices** selon leur pertinence Ã  lâ€™aide de **modÃ¨les basÃ©s sur BERT**.

### ğŸš€ ExÃ©cuter la DÃ©mo HintRank sur Google Colab

Vous pouvez facilement essayer **HintRank** dans votre navigateur via **Google Colab**, sans installation locale requise. Lancez simplement le **[notebook Colab](https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb)** pour explorer **HintRank** de maniÃ¨re interactive.

### 1ï¸âƒ£ Installer les dÃ©pendances

Si vous exÃ©cutez HintRank localement, assurez-vous dâ€™avoir installÃ© les dÃ©pendances requises :

```sh
pip install transformers torch numpy scipy
```

### 2ï¸âƒ£ Importer et initialiser HintRank

AccÃ©dez au rÃ©pertoire `HintRank` et importez le module `hint_rank` :

```python
from HintRank.hint_rank import HintRank

# Initialiser le modÃ¨le HintRank
ranker = HintRank()
```

### 3ï¸âƒ£ Classer les indices pour une question donnÃ©e

```python
question = "Quelle est la capitale de l'Autriche ?"
answer = "Vienne"
hints = [
    "Mozart et Beethoven y ont vÃ©cu.",
    "C'est une grande ville en Europe.",
    "La plus grande ville d'Autriche."
]

# Exemple de comparaison par paires
better_hint_answer_aware = ranker.pairwise_compare(question, hints[1], hints[2], answer)
better_hint_answer_agnostic = ranker.pairwise_compare(question, hints[0], hints[1])

print(f"RÃ©ponse-Consciente : L'indice {2 if better_hint_answer_aware == 1 else 3} est meilleur que l'indice {3 if better_hint_answer_aware == 0 else 2}.")
print(f"RÃ©ponse-Agnostique : L'indice {1 if better_hint_answer_agnostic == 1 else 2} est meilleur que l'indice {2 if better_hint_answer_agnostic == 0 else 1}.")
```

### 4ï¸âƒ£ Classement des indices en liste

Vous pouvez Ã©galement classer plusieurs indices en une seule fois :

```python
print("\nIndices classÃ©s (RÃ©ponse-Consciente) :")
ranked_hints_answer_aware = ranker.listwise_compare(question, hints, answer)
for i, (hint, _) in enumerate(ranked_hints_answer_aware):
    print(f"Rang {i + 1} : {hint}")

print("\nIndices classÃ©s (RÃ©ponse-Agnostique) :")
ranked_hints_answer_agnostic = ranker.listwise_compare(question, hints)
for i, (hint, _) in enumerate(ranked_hints_answer_agnostic):
    print(f"Rang {i + 1} : {hint}")
```

### ğŸ“Œ RÃ©sultat Attendu

```
Comparaison par Paires des Indices
	RÃ©ponse-Consciente : L'indice 3 est meilleur que l'indice 2.
	RÃ©ponse-Agnostique : L'indice 2 est meilleur que l'indice 1.

Classement des Indices en Liste
	Indices ClassÃ©s (RÃ©ponse-Consciente) :
		Rang 1 : La plus grande ville d'Autriche.
		Rang 2 : Mozart et Beethoven y ont vÃ©cu.
		Rang 3 : C'est une grande ville en Europe.

	Indices ClassÃ©s (RÃ©ponse-Agnostique) :
		Rang 1 : C'est une grande ville en Europe.
		Rang 2 : La plus grande ville d'Autriche.
		Rang 3 : Mozart et Beethoven y ont vÃ©cu.
```

---

## ğŸ“Š ğŸ†š Comparaison du Jeu de DonnÃ©es WikiHint vs. TriviaHG

Le tableau ci-dessous compare **WikiHint** avec **TriviaHG**, le plus grand jeu de donnÃ©es prÃ©cÃ©dent pour la gÃ©nÃ©ration d'indices. WikiHint offre **une meilleure convergence**, des **indices plus courts** et des **indices de meilleure qualitÃ©** selon plusieurs mÃ©triques d'Ã©valuation.

| **Jeu de DonnÃ©es** | **Sous-ensemble** | **Pertinence** | **LisibilitÃ©** | **Convergence** | **FamiliaritÃ©** | **Longueur** | **Fuite de RÃ©ponse (Moyenne)** | **Fuite de RÃ©ponse (Max.)** |
|--------------------|------------------|---------------|---------------|---------------|---------------|----------|------------------|------------------|
| TriviaHG          | Complet          | 0.95          | 0.71          | 0.57          | 0.77          | 20.82    | 0.23             | 0.44             |
| WikiHint          | Complet          | 0.98          | 0.72          | 0.73          | 0.75          | 17.82    | 0.24             | 0.49             |
| TriviaHG          | EntraÃ®nement      | 0.95          | 0.73          | 0.57          | 0.75          | 21.19    | 0.22             | 0.44             |
| WikiHint          | EntraÃ®nement      | 0.98          | 0.71          | 0.74          | 0.76          | 17.77    | 0.24             | 0.49             |
| TriviaHG          | Test              | 0.95          | 0.73          | 0.60          | 0.77          | 20.97    | 0.23             | 0.44             |
| WikiHint          | Test              | 0.98          | 0.83          | 0.72          | 0.73          | 18.32    | 0.24             | 0.47             |

ğŸ“Œ **Principaux RÃ©sultats** :
- **WikiHint surpasse TriviaHG** en **convergence**, ce qui signifie que ses indices aident les utilisateurs **Ã  trouver les rÃ©ponses plus efficacement**.
- **Les indices de WikiHint sont plus courts**, ce qui permet une **guidance plus concise et efficace**.

---

## ğŸ“ŠğŸ¤– Ã‰valuation des Indices GÃ©nÃ©rÃ©s

Ce tableau prÃ©sente une **Ã©valuation des indices gÃ©nÃ©rÃ©s** par diffÃ©rents **LLMs (LLaMA-3.1, GPT-4)** en fonction de **la pertinence, la lisibilitÃ©, la convergence, la familiaritÃ©, la longueur des indices et la fuite de rÃ©ponse**. Il met en Ã©vidence l'impact du **finetuning (FT)** et de la **prise en compte de la rÃ©ponse (wA)** sur la qualitÃ© des indices.

| **ModÃ¨le** | **Config** | **Utilise la RÃ©ponse ?** | **Pert.** | **Lisib.** | **Conv. (LLaMA-8B)** | **Conv. (LLaMA-70B)** | **Fam.** | **Long.** | **Fuite (Moyenne)** | **Fuite (Max.)** |
|------------|-----------|----------------|-----------|-----------|------------------|------------------|-----------|---------|----------------|----------------|
| **GPT-4**    | Vanilla  | âœ…      | 0.91         | 1.00           | 0.14             | 0.48             | 0.84         | 26.36   | 0.23           | 0.51           |
| **GPT-4**    | Vanilla  | âŒ      | 0.92         | 1.10           | 0.12             | 0.47             | 0.81         | 26.93   | 0.24           | 0.52           |
| **LLaMA-3.1-405b** | Vanilla  | âœ… | 0.94         | 1.49           | 0.11             | 0.47             | 0.76         | 41.81   | 0.23           | 0.50           |
| **LLaMA-3.1-405b** | Vanilla  | âŒ| 0.92         | 1.53           | 0.10             | 0.45             | 0.78         | 50.91   | 0.23           | 0.50           |
| **LLaMA-3.1-70b**  | FTwA    | âœ… | 0.88         | 1.50           | 0.09             | 0.42             | 0.84         | 43.69   | 0.22           | 0.48           |
| **LLaMA-3.1-70b**  | Vanilla  | âœ… | 0.86         | 1.53           | 0.05             | 0.42             | 0.80         | 45.51   | 0.23           | 0.50           |
| **LLaMA-3.1-70b**  | FTwoA   | âŒ | 0.86         | 1.50           | 0.08             | 0.38             | 0.80         | 51.07   | 0.22           | 0.51           |
| **LLaMA-3.1-70b**  | Vanilla  | âŒ | 0.87         | 1.56           | 0.06             | 0.38             | 0.76         | 53.24   | 0.22           | 0.50           |
| **LLaMA-3.1-8b**   | FTwA    | âœ… | 0.78         | 1.63           | 0.05             | 0.37             | 0.79         | 50.33   | 0.22           | 0.52           |
| **LLaMA-3.1-8b**   | Vanilla  | âœ… | 0.81         | 1.72           | 0.05             | 0.32             | 0.80         | 54.38   | 0.22           | 0.50           |
| **LLaMA-3.1-8b**   | FTwoA   | âŒ | 0.76         | 1.70           | 0.03             | 0.32             | 0.80         | 55.02   | 0.22           | 0.51           |
| **LLaMA-3.1-8b**   | Vanilla  | âŒ | 0.78         | 1.76           | 0.04             | 0.30             | 0.83         | 52.99   | 0.22           | 0.50           |

ğŸ“Œ **Principaux Enseignements** :
- **Pertinence** : **Les modÃ¨les plus grands (405b, 70b) produisent de meilleurs indices** que les modÃ¨les plus petits (8b).
- **LisibilitÃ©** : **GPT-4 gÃ©nÃ¨re les indices les plus lisibles**.
- **Convergence** : **Les indices prenant en compte la rÃ©ponse (wA) aident les LLMs Ã  gÃ©nÃ©rer de meilleurs indices**.
- **FamiliaritÃ©** : Les modÃ¨les plus grands gÃ©nÃ¨rent **des indices plus familiers**, basÃ©s sur des connaissances gÃ©nÃ©rales.
- **Longueur des indices** : **Les modÃ¨les ajustÃ©s finement (FTwA, FTwoA) gÃ©nÃ¨rent des indices plus courts et plus efficaces**.

---

## ğŸ“‚ Structure du DÃ©pÃ´t

```
ğŸ“‚ WikiHint/                                                # ğŸ—‚ Fichiers du jeu de donnÃ©es
â”‚   â”œâ”€â”€ ğŸ“„ Pipeline.png                                     # ğŸ“Š Visualisation du pipeline du jeu de donnÃ©es
â”‚   â”œâ”€â”€ ğŸ“„ training.json                                    # ğŸ“Š Ensemble d'entraÃ®nement (900 questions, 4500 indices)
â”‚   â”œâ”€â”€ ğŸ“„ test.json                                        # ğŸ“Š Ensemble de test (100 questions, 500 indices)
â”‚
â”œâ”€â”€ ğŸ“‚ Experiments/                                         # ğŸ§ª Indices gÃ©nÃ©rÃ©s par les modÃ¨les
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-agnostic.json
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-aware.json
â”‚
â”œâ”€â”€ ğŸ“‚ HintRank/                                            # ğŸ† MÃ©thode de classement des indices
â”‚   â”œâ”€â”€ ğŸ“„ EvaluationMethod.png                             # ğŸ“Š Visualisation de lâ€™Ã©valuation de HintRank
â”‚   â”œâ”€â”€ ğŸ“„ hint_rank.py                                     # ğŸ“œ ImplÃ©mentation de HintRank
â”‚
â”œâ”€â”€ ğŸ“‚ HumanEvaluation/                                     # ğŸ‘¨â€ğŸ”¬ DonnÃ©es dâ€™Ã©valuation humaine
â”‚   â”œâ”€â”€ ğŸ“‚ Person_1/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_1.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_2.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_3.xlsx
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ Person_2/ (MÃªme structure que Person_1)
â”‚   â”œâ”€â”€ ğŸ“‚ Person_3/ (MÃªme structure que Person_1)
â”‚
â””â”€â”€ ğŸ“˜ README.md                                            # ğŸ“– Ce fichier
```

---

## ğŸ“œ Licence

Ce projet est sous licence **Creative Commons Attribution 4.0 International (CC BY 4.0)**. Vous Ãªtes libre d'utiliser, partager et adapter le jeu de donnÃ©es avec attribution.

## ğŸ“‘ Citation

Si ce travail vous est utile, merci de citer [ğŸ“œnotre article](https://doi.org/10.1145/3726302.3730284) :

Jamshid Mozafari, Florian Gerhold, and Adam Jatowt. 2025. WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '25). Association for Computing Machinery, New York, NY, USA, 3821â€“3831. https://doi.org/10.1145/3726302.3730284

### ğŸ“„ BibTeX:
```bibtex
@inproceedings{10.1145/3726302.3730284,
	author = {Mozafari, Jamshid and Gerhold, Florian and Jatowt, Adam},
	title = {WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation},
	year = {2025},
	isbn = {9798400715921},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3726302.3730284},
	doi = {10.1145/3726302.3730284},
	booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages = {3821â€“3831},
	numpages = {11},
	keywords = {hint dataset, hint evaluation, hint generation, hint ranking},
	location = {Padua, Italy},
	series = {SIGIR '25}
}
```

---

## ğŸ™ Remerciements

Merci Ã  nos contributeurs et Ã  l'UniversitÃ© d'Innsbruck pour leur soutien Ã  ce projet.
