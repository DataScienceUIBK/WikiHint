[English](README.md) | [ä¸­æ–‡](README.zh.md) | [Deutsch](README.de.md) | [FranÃ§ais](README.fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)

# WikiHint : Un Jeu de DonnÃ©es AnnotÃ© par des Humains pour le Classement et la GÃ©nÃ©ration d'Indices

<a href="https://doi.org/10.48550/arXiv.2412.01626"><img src="https://img.shields.io/static/v1?label=Article&message=arXiv&color=green&logo=arxiv"></a>
<a href="https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb"><img src="https://img.shields.io/static/v1?label=Colab&message=Demo&logo=Google%20Colab&color=f9ab00"></a>
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-blue)](https://creativecommons.org/licenses/by/4.0/)
<img src="https://github.com/DataScienceUIBK/WikiHint/blob/main/WikiHint/Pipeline.png">

WikiHint est un **jeu de donnÃ©es annotÃ© par des humains** conÃ§u pour la **gÃ©nÃ©ration et le classement automatiques d'indices** pour les questions factuelles. Ce jeu de donnÃ©es, basÃ© sur Wikipedia, contient **5 000 indices pour 1 000 questions** et soutient la recherche sur **lâ€™Ã©valuation, le classement et la gÃ©nÃ©ration dâ€™indices**.

## ğŸ—‚ Vue dâ€™ensemble

- **1 000 questions** avec **5 000 indices crÃ©Ã©s manuellement**.
- Indices classÃ©s par des **annotateurs humains** selon leur utilitÃ©.
- Ã‰valuÃ© Ã  l'aide de **LLMs (LLaMA, GPT-4)** et dâ€™**Ã©tudes sur la performance humaine**.
- Supporte le **classement des indices** et **lâ€™Ã©valuation automatique des indices**.

## ğŸ”¬ Contributions Ã  la recherche

âœ… **Premier jeu de donnÃ©es annotÃ© par des humains** pour la gÃ©nÃ©ration et le classement d'indices.  
âœ… **HintRank** : Une mÃ©thode lÃ©gÃ¨re pour le classement automatique des indices.  
âœ… **Ã‰tude humaine** Ã©valuant lâ€™efficacitÃ© des indices pour aider les utilisateurs Ã  rÃ©pondre aux questions.  
âœ… **Ajustement fin des LLM open-source** (LLaMA-3.1, GPT-4) pour la gÃ©nÃ©ration dâ€™indices.  

## ğŸ“ˆ Principaux enseignements

- **Les indices tenant compte de la rÃ©ponse** amÃ©liorent leur efficacitÃ©.  
- **Les modÃ¨les LLaMA ajustÃ©s finement** gÃ©nÃ¨rent de meilleurs indices que les modÃ¨les bruts.  
- **Les indices plus courts** ont tendance Ã  Ãªtre **plus efficaces** que les plus longs.  
- **Les indices gÃ©nÃ©rÃ©s par des humains** surpassent ceux des LLM en termes de clartÃ© et de classement.

## ğŸš€ Prise en main

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t

```sh
git clone https://github.com/DataScienceUIBK/WikiHint.git
cd WikiHint
```

### 2ï¸âƒ£ Charger le jeu de donnÃ©es

```python
import json

with open("./WikiHint/training.json", "r") as f:
    training_data = json.load(f)

with open("./WikiHint/test.json", "r") as f:
    test_data = json.load(f)

print(f"Ensemble dâ€™entraÃ®nement : {len(training_data)} questions")
print(f"Ensemble de test : {len(test_data)} questions")
```

## ğŸ† HintRank : Une MÃ©thode LÃ©gÃ¨re de Classement des Indices

HintRank est une **mÃ©thode de classement automatique** des indices utilisant des **modÃ¨les basÃ©s sur BERT**. Il fonctionne sur le principe des **comparaisons par paires**, dÃ©terminant la **pertinence relative des indices**.

<p align="center">
    <img src="https://raw.githubusercontent.com/DataScienceUIBK/WikiHint/main/HintRank/EvaluationMethod.png" width="35%">
</p>

### âœ¨ FonctionnalitÃ©s :
âœ” **LÃ©ger** : Fonctionne localement sans nÃ©cessiter de ressources informatiques massives.  
âœ” **Ã‰valuation sans LLM** : Fonctionne sans dÃ©pendre de **grands modÃ¨les gÃ©nÃ©ratifs**.  
âœ” **Classement alignÃ© sur les humains** : Forte corrÃ©lation avec le **classement des indices Ã©tabli par des humains**.  

### ğŸ” Comment Ã§a marche ?
1. **ConcatÃ¨ne la question & deux indices** â†’ Les convertit au format compatible BERT.  
2. **Calcule la qualitÃ© des indices** â†’ DÃ©termine quel indice est **le plus utile**.  
3. **GÃ©nÃ¨re un classement des indices** â†’ Attribue un rang basÃ© sur des comparaisons par paires.  

## ğŸ“Œ Utilisation de `HintRank` pour le Classement des Indices

Le module `HintRank` est conÃ§u pour **classer automatiquement les indices** selon leur pertinence Ã  lâ€™aide de **modÃ¨les basÃ©s sur BERT**.

### ğŸš€ ExÃ©cuter la DÃ©mo HintRank sur Google Colab

Vous pouvez facilement essayer **HintRank** dans votre navigateur via **Google Colab**, sans installation locale requise. Lancez simplement le **[notebook Colab](https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb)** pour explorer **HintRank** de maniÃ¨re interactive.

### 1ï¸âƒ£ Installer les dÃ©pendances

Si vous exÃ©cutez HintRank localement, assurez-vous dâ€™avoir installÃ© les dÃ©pendances requises :

```sh
pip install transformers torch numpy scipy
```

### 2ï¸âƒ£ Importer et initialiser HintRank

AccÃ©dez au rÃ©pertoire `HintRank` et importez le module `hint_rank` :

```python
from HintRank.hint_rank import HintRank

# Initialiser le modÃ¨le HintRank
ranker = HintRank()
```

### 3ï¸âƒ£ Classer les indices pour une question donnÃ©e

```python
question = "Quelle est la capitale de l'Autriche ?"
answer = "Vienne"
hints = [
    "Mozart et Beethoven y ont vÃ©cu.",
    "C'est une grande ville en Europe.",
    "La plus grande ville d'Autriche."
]

# Exemple de comparaison par paires
better_hint_answer_aware = ranker.pairwise_compare(question, hints[1], hints[2], answer)
better_hint_answer_agnostic = ranker.pairwise_compare(question, hints[0], hints[1])

print(f"RÃ©ponse-Consciente : L'indice {2 if better_hint_answer_aware == 1 else 3} est meilleur que l'indice {3 if better_hint_answer_aware == 0 else 2}.")
print(f"RÃ©ponse-Agnostique : L'indice {1 if better_hint_answer_agnostic == 1 else 2} est meilleur que l'indice {2 if better_hint_answer_agnostic == 0 else 1}.")
```

### 4ï¸âƒ£ Classement des indices en liste

Vous pouvez Ã©galement classer plusieurs indices en une seule fois :

```python
print("\nIndices classÃ©s (RÃ©ponse-Consciente) :")
ranked_hints_answer_aware = ranker.listwise_compare(question, hints, answer)
for i, (hint, _) in enumerate(ranked_hints_answer_aware):
    print(f"Rang {i + 1} : {hint}")

print("\nIndices classÃ©s (RÃ©ponse-Agnostique) :")
ranked_hints_answer_agnostic = ranker.listwise_compare(question, hints)
for i, (hint, _) in enumerate(ranked_hints_answer_agnostic):
    print(f"Rang {i + 1} : {hint}")
```

---

### ğŸ“Œ RÃ©sultat Attendu

```
Comparaison par Paires des Indices
	RÃ©ponse-Consciente : L'indice 3 est meilleur que l'indice 2.
	RÃ©ponse-Agnostique : L'indice 2 est meilleur que l'indice 1.

Classement des Indices en Liste
	Indices ClassÃ©s (RÃ©ponse-Consciente) :
		Rang 1 : La plus grande ville d'Autriche.
		Rang 2 : Mozart et Beethoven y ont vÃ©cu.
		Rang 3 : C'est une grande ville en Europe.

	Indices ClassÃ©s (RÃ©ponse-Agnostique) :
		Rang 1 : C'est une grande ville en Europe.
		Rang 2 : La plus grande ville d'Autriche.
		Rang 3 : Mozart et Beethoven y ont vÃ©cu.
```

---

## ğŸ“Š ğŸ†š Comparaison du Jeu de DonnÃ©es WikiHint vs. TriviaHG

Le tableau ci-dessous compare **WikiHint** avec **TriviaHG**, le plus grand jeu de donnÃ©es prÃ©cÃ©dent pour la gÃ©nÃ©ration d'indices. WikiHint offre **une meilleure convergence**, des **indices plus courts** et des **indices de meilleure qualitÃ©** selon plusieurs mÃ©triques d'Ã©valuation.

| **Jeu de DonnÃ©es** | **Sous-ensemble** | **Pertinence** | **LisibilitÃ©** | **Convergence** | **FamiliaritÃ©** | **Longueur** | **Fuite de RÃ©ponse (Moyenne)** | **Fuite de RÃ©ponse (Max.)** |
|--------------------|------------------|---------------|---------------|---------------|---------------|----------|------------------|------------------|
| TriviaHG          | Complet          | 0.95          | 0.71          | 0.57          | 0.77          | 20.82    | 0.23             | 0.44             |
| WikiHint          | Complet          | 0.98          | 0.72          | 0.73          | 0.75          | 17.82    | 0.24             | 0.49             |
| TriviaHG          | EntraÃ®nement      | 0.95          | 0.73          | 0.57          | 0.75          | 21.19    | 0.22             | 0.44             |
| WikiHint          | EntraÃ®nement      | 0.98          | 0.71          | 0.74          | 0.76          | 17.77    | 0.24             | 0.49             |
| TriviaHG          | Test              | 0.95          | 0.73          | 0.60          | 0.77          | 20.97    | 0.23             | 0.44             |
| WikiHint          | Test              | 0.98          | 0.83          | 0.72          | 0.73          | 18.32    | 0.24             | 0.47             |

ğŸ“Œ **Principaux RÃ©sultats** :
- **WikiHint surpasse TriviaHG** en **convergence**, ce qui signifie que ses indices aident les utilisateurs **Ã  trouver les rÃ©ponses plus efficacement**.
- **Les indices de WikiHint sont plus courts**, ce qui permet une **guidance plus concise et efficace**.

---

## ğŸ“ŠğŸ¤– Ã‰valuation des Indices GÃ©nÃ©rÃ©s

Ce tableau prÃ©sente une **Ã©valuation des indices gÃ©nÃ©rÃ©s** par diffÃ©rents **LLMs (LLaMA-3.1, GPT-4)** en fonction de **la pertinence, la lisibilitÃ©, la convergence, la familiaritÃ©, la longueur des indices et la fuite de rÃ©ponse**. Il met en Ã©vidence l'impact du **finetuning (FT)** et de la **prise en compte de la rÃ©ponse (wA)** sur la qualitÃ© des indices.

| **ModÃ¨le** | **Config** | **Utilise la RÃ©ponse ?** | **Pert.** | **Lisib.** | **Conv. (LLaMA-8B)** | **Conv. (LLaMA-70B)** | **Fam.** | **Long.** | **Fuite (Moyenne)** | **Fuite (Max.)** |
|------------|-----------|----------------|-----------|-----------|------------------|------------------|-----------|---------|----------------|----------------|
| **GPT-4**  | Vanilla   | âœ…              | 0.91      | 1.00      | 0.14             | 0.48             | 0.84      | 26.36   | 0.23           | 0.51           |
| **GPT-4**  | Vanilla   | âŒ              | 0.92      | 1.10      | 0.12             | 0.47             | 0.81      | 26.93   | 0.24           | 0.52           |
| **LLaMA-3.1-405b** | Vanilla | âœ…     | 0.94      | 1.49      | 0.11             | 0.47             | 0.76      | 41.81   | 0.23           | 0.50           |
| **LLaMA-3.1-405b** | Vanilla | âŒ     | 0.92      | 1.53      | 0.10             | 0.45             | 0.78      | 50.91   | 0.23           | 0.50           |

ğŸ“Œ **Principaux Enseignements** :
- **Pertinence** : **Les modÃ¨les plus grands (405b, 70b) produisent de meilleurs indices** que les modÃ¨les plus petits (8b).
- **LisibilitÃ©** : **GPT-4 gÃ©nÃ¨re les indices les plus lisibles**.
- **Convergence** : **Les indices prenant en compte la rÃ©ponse (wA) aident les LLMs Ã  gÃ©nÃ©rer de meilleurs indices**.
- **FamiliaritÃ©** : Les modÃ¨les plus grands gÃ©nÃ¨rent **des indices plus familiers**, basÃ©s sur des connaissances gÃ©nÃ©rales.
- **Longueur des indices** : **Les modÃ¨les ajustÃ©s finement (FTwA, FTwoA) gÃ©nÃ¨rent des indices plus courts et plus efficaces**.

---

## ğŸ“‚ Structure du DÃ©pÃ´t

```
ğŸ“‚ WikiHint/                                                # ğŸ—‚ Fichiers du jeu de donnÃ©es
â”‚   â”œâ”€â”€ ğŸ“„ Pipeline.png                                     # ğŸ“Š Visualisation du pipeline du jeu de donnÃ©es
â”‚   â”œâ”€â”€ ğŸ“„ training.json                                    # ğŸ“Š Ensemble d'entraÃ®nement (900 questions, 4500 indices)
â”‚   â”œâ”€â”€ ğŸ“„ test.json                                        # ğŸ“Š Ensemble de test (100 questions, 500 indices)
â”‚
â”œâ”€â”€ ğŸ“‚ Experiments/                                         # ğŸ§ª Indices gÃ©nÃ©rÃ©s par les modÃ¨les
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-agnostic.json
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-aware.json
â”‚
â”œâ”€â”€ ğŸ“‚ HintRank/                                            # ğŸ† MÃ©thode de classement des indices
â”‚   â”œâ”€â”€ ğŸ“„ EvaluationMethod.png                             # ğŸ“Š Visualisation de lâ€™Ã©valuation de HintRank
â”‚   â”œâ”€â”€ ğŸ“„ hint_rank.py                                     # ğŸ“œ ImplÃ©mentation de HintRank
â”‚
â”œâ”€â”€ ğŸ“‚ HumanEvaluation/                                     # ğŸ‘¨â€ğŸ”¬ DonnÃ©es dâ€™Ã©valuation humaine
â”‚   â”œâ”€â”€ ğŸ“‚ Person_1/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_1.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_2.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_3.xlsx
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ Person_2/ (MÃªme structure que Person_1)
â”‚   â”œâ”€â”€ ğŸ“‚ Person_3/ (MÃªme structure que Person_1)
â”‚
â””â”€â”€ ğŸ“˜ README.md                                            # ğŸ“– Ce fichier
```

---

## ğŸ“œ Licence

Ce projet est sous licence **Creative Commons Attribution 4.0 International (CC BY 4.0)**. Vous Ãªtes libre d'utiliser, partager et adapter le jeu de donnÃ©es avec attribution.

## ğŸ“‘ Citation

Si ce travail vous est utile, merci de citer [ğŸ“œnotre article](https://doi.org/10.48550/arXiv.2412.01626) :

### ğŸ“„ BibTeX :
```bibtex
@article{mozafari2025wikihinthumanannotateddatasethint,
      title={WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation}, 
      author={Jamshid Mozafari and Florian Gerhold and Adam Jatowt},
      year={2025},
      eprint={2412.01626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2412.01626}, 
}
```

---

## ğŸ™ Remerciements

Merci Ã  nos contributeurs et Ã  l'UniversitÃ© d'Innsbruck pour leur soutien Ã  ce projet.
