[English](README.md) | [ä¸­æ–‡](README.zh.md) | [Deutsch](README.de.md) | [FranÃ§ais](README.fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)

# WikiHint: Ein von Menschen annotierter Datensatz fÃ¼r Hinweisbewertung und -generierung
<a href="https://huggingface.co/datasets/JamshidJDMY/WikiHint"><img alt="Huggingface" src="https://img.shields.io/static/v1?label=Dataset&message=WikiHint&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAIoUExURQAAAP/////57v/67xUVFf/clv+KAP/uzf/sxv8AAP9TAP/ltP///v/ouP/////////////////////////////8+v/pvP/biP/Vbf/Vbv/cif/qv//9+//////////03v/Yev/Zfv/14//25v/Uav/Vbv/46//////dkf/gmP/////04P/25//pvP/sxf/////lr//ouP/qwP/tyf/msv/ntf/+/P/36f/LUf/36P/w0v/w0//w0//w0//78//gm//QZv/RZv/gm//78v/////14v/nt//gnP/hn//w0f/////w0f/hn//gnP/nt//14v/////////////////LLv/MGv/PGf/LL//LG//THP/THv/SHv/UHv/LGv/LH/7SHuK8JOzDIuW+I+jAI//LHv/PTP/NF/PBG3BkOpyGMvrOH4R0NoV0NvzJGv/MF//QT//MLv/LGPu/FNayJu7FIdq2Jf7DFP/JF//ML//LJurCIsCiKujCIubAI7+hK/DHIf/LJ//HK//NGf/SHeS9IlxSP25QQmtOQmVZPu3DIf/RHf/HLP++Kf/AD/++EP3EFNCfLNhpQthrQdinKv/FFP/AEP+/K/++Dv/BEv+/Ef/CE//MIf/NIP/MGf++D//KTP/FOP/DE//PG//PHP/JGP/EFP/EM//BDf/TH//GFP/CEP/DEP/EEv/BDv/MS//IJ//JHf/JHP/JP//IQf/IHP/IJv/LSf///7SHOh0AAABUdFJOUwAAAAAAAAAAAAAAAAAABiZCQykIAUGn3/Hy4q5KAwRy7vJ/Yfb7cR/X4ipkdpepAqi5mavM2z5v/pGTtZS2QtP4999bIGyry8yUR4fJzbJ2BRIRE9ZoIHEAAAABYktHRAH/Ai3eAAAAB3RJTUUH6AIGEyohVAr+rAAAARZJREFUGNNjYGBgZGTi4xcQFBJmZmRkYQDxRUTFxCUkpaRlZBkZQXw5eYWQ0LCw0HBFJWGgCCOrskpEZFR0dExkrKoaG1BAXSMuPiExOjopOSpFU4uRgV07NS09IzMqKzsnNy9fh4OBU7egsKi4JCo6ubSsvEJPlkHfoDIqqqq6prauPiqqwVCYgcuosam5pbWtvaOzq6nbWJZBy6Snt69/wsRJk6f0TZ1masZgbjF9xsxZhbPnzJ01c8a8+ZYMVgt6F5aHLly0eGHokqVTl1kz2CxYXhi1YuWq1WuiouauXWbLYGe/bv2GjZscHDdv2bh1m5MzA7eLq5u7h6eXoLePr5+/ENDpjBwBgYH6PIyMQcF8vIyMAKnZUpQQgaV4AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDI0LTAyLTA2VDE5OjQyOjI1KzAwOjAwybP6HAAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyNC0wMi0wNlQxOTo0MjoyNSswMDowMLjuQqAAAAAodEVYdGRhdGU6dGltZXN0YW1wADIwMjQtMDItMDZUMTk6NDI6MzMrMDA6MDBAgVbbAAAAAElFTkSuQmCC&color=20BEFF"/></a>
<a href="https://doi.org/10.1145/3726302.3730284"><img src="https://img.shields.io/static/v1?label=Paper&message=ACM%20SIGIR&color=green&logo=arxiv"></a>
<a href="https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb"><img src="https://img.shields.io/static/v1?label=Colab&message=Demo&logo=Google%20Colab&color=f9ab00"></a>
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-blue)](https://creativecommons.org/licenses/by/4.0/)
<img src="https://github.com/DataScienceUIBK/WikiHint/blob/main/WikiHint/Pipeline.png">

WikiHint ist ein **von Menschen annotierter Datensatz**, der fÃ¼r die **automatische Generierung und Bewertung von Hinweisen** bei Faktfragen entwickelt wurde. Der Datensatz basiert auf Wikipedia und enthÃ¤lt **5.000 Hinweise zu 1.000 Fragen**. Er unterstÃ¼tzt die Forschung in den Bereichen **Hinweisbewertung, -ranking und -generierung**.

## <img src="https://github.com/DataScienceUIBK/TriviaHG/blob/main/Framework/gif-dan.gif" width="32" height="32"/> Achtung <img src="https://github.com/DataScienceUIBK/TriviaHG/blob/main/Framework/gif-dan.gif" width="32" height="32"/>

Ab **Februar 2025** empfehlen wir die Nutzung von **HintEval**, dem Framework fÃ¼r **Hinweiserzeugung und -bewertung**. HintEval enthÃ¤lt den **WikiHint-Datensatz** sowie die Evaluationsmetriken aus der WikiHint-Publikation und macht die Arbeit mit Hinweisen so einfach wie nie zuvor.

Hier gehtâ€™s zu HintEval:  
- ğŸ“– **[HintEval-Dokumentation](http://hinteval.readthedocs.io/)**  
- ğŸ“¦ **[HintEval PyPI-Installation](https://pypi.org/project/hinteval/)**  
- ğŸ’» **[HintEval GitHub-Repository](https://github.com/DataScienceUIBK/HintEval)**  
- ğŸ“œ **[HintEval-Paper (arXiv)](https://doi.org/10.48550/arXiv.2502.00857)**  

FÃ¼r eine **nahtlose Integration** von Hinweiserzeugung und -bewertung empfehlen wir dringend den **Umstieg** auf **HintEval**!

## ğŸ—‚ Ãœberblick

- **1.000 Fragen** mit **5.000 manuell erstellten Hinweisen**.
- Hinweise werden von **menschlichen Annotatoren** nach NÃ¼tzlichkeit bewertet.
- Evaluierung durch **LLMs (LLaMA, GPT-4)** und **Studien zur menschlichen Leistung**.
- UnterstÃ¼tzt **Hinweisranking** und **automatische Hinweisbewertung**.

## ğŸ”¬ Wissenschaftliche BeitrÃ¤ge

âœ… **Erster von Menschen annotierter Datensatz** fÃ¼r die Generierung und Bewertung von Hinweisen.  
âœ… **HintRank:** Eine leichtgewichtige Methode zur automatischen Hinweisbewertung.  
âœ… **Human-Studie** zur Bewertung der EffektivitÃ¤t von Hinweisen fÃ¼r Benutzer.  
âœ… **Feinabstimmung von Open-Source-LLMs** (LLaMA-3.1, GPT-4) zur Hinweisgenerierung.  

## ğŸ“ˆ Zentrale Erkenntnisse

- **Antwortbewusste Hinweise** verbessern die EffektivitÃ¤t.  
- **Feinabgestimmte LLaMA-Modelle** erzeugen bessere Hinweise als Standardmodelle.  
- **KÃ¼rzere Hinweise** sind tendenziell **effektiver** als lÃ¤ngere.  
- **Von Menschen erstellte Hinweise** Ã¼bertreffen LLM-generierte Hinweise in Klarheit und Ranking.

## ğŸš€ Erste Schritte

### 1ï¸âƒ£ Repository klonen

```sh
git clone https://github.com/DataScienceUIBK/WikiHint.git
cd WikiHint
```

### 2ï¸âƒ£ Datensatz laden

```python
import json

with open("./WikiHint/training.json", "r") as f:
    training_data = json.load(f)

with open("./WikiHint/test.json", "r") as f:
    test_data = json.load(f)

print(f"Trainingsset: {len(training_data)} Fragen")
print(f"Testset: {len(test_data)} Fragen")
```

## ğŸ† HintRank: Eine leichtgewichtige Methode zur Hinweisbewertung

HintRank ist eine **automatische Methode zur Bewertung von Hinweisen**, die auf **BERT-basierten Modellen** beruht. Sie nutzt **paarweise Vergleiche**, um die **relative NÃ¼tzlichkeit von Hinweisen** zu bestimmen.

<p align="center">
    <img src="https://raw.githubusercontent.com/DataScienceUIBK/WikiHint/main/HintRank/EvaluationMethod.png" width="35%">
</p>

### âœ¨ Funktionen:
âœ” **Leichtgewichtig**: LÃ¤uft lokal ohne hohen Rechenaufwand.  
âœ” **LLM-freie Bewertung**: Funktioniert ohne **groÃŸe generative Modelle**.  
âœ” **An Menschen ausgerichtetes Ranking**: Hohe Korrelation mit **von Menschen zugewiesenen Hinweisbewertungen**.  

### ğŸ” Funktionsweise:
1. **VerknÃ¼pft Frage & zwei Hinweise** â†’ Konvertiert sie in ein BERT-kompatibles Format.  
2. **Berechnet die QualitÃ¤t der Hinweise** â†’ Bestimmt, welcher Hinweis **nÃ¼tzlicher** ist.  
3. **Erstellt ein Hinweisranking** â†’ Vergibt Platzierungen basierend auf paarweisen Vergleichen.  

## ğŸ“Œ Nutzung von `HintRank` zur Hinweisbewertung

Das `HintRank`-Modul dient zur **automatischen Bewertung von Hinweisen** anhand ihrer NÃ¼tzlichkeit mit **BERT-basierten Modellen**.

### ğŸš€ HintRank-Demo in Google Colab ausfÃ¼hren

Du kannst **HintRank** einfach in deinem Browser Ã¼ber **Google Colab** ausprobieren, ohne eine lokale Installation vornehmen zu mÃ¼ssen. Starte einfach das **[Colab-Notebook](https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb)** und erkunde **HintRank** interaktiv.

### 1ï¸âƒ£ AbhÃ¤ngigkeiten installieren

Falls du `HintRank` lokal ausfÃ¼hrst, installiere die erforderlichen AbhÃ¤ngigkeiten:

```sh
pip install transformers torch numpy scipy
```

### 2ï¸âƒ£ HintRank importieren und initialisieren

Navigiere in das `HintRank`-Verzeichnis und importiere das `hint_rank`-Modul:

```python
from HintRank.hint_rank import HintRank

# HintRank-Modell initialisieren
ranker = HintRank()
```

### 3ï¸âƒ£ Hinweise fÃ¼r eine Frage bewerten

```python
question = "Was ist die Hauptstadt von Ã–sterreich?"
answer = "Wien"
hints = [
    "Mozart und Beethoven haben hier gelebt.",
    "Es ist eine groÃŸe Stadt in Europa.",
    "Ã–sterreichs grÃ¶ÃŸte Stadt."
]

# Beispiel fÃ¼r paarweisen Vergleich
better_hint_answer_aware = ranker.pairwise_compare(question, hints[1], hints[2], answer)
better_hint_answer_agnostic = ranker.pairwise_compare(question, hints[0], hints[1])

print(f"Antwortbewusst: Hinweis {2 if better_hint_answer_aware == 1 else 3} ist besser als Hinweis {3 if better_hint_answer_aware == 0 else 2}.")
print(f"Antwortagnostisch: Hinweis {1 if better_hint_answer_agnostic == 1 else 2} ist besser als Hinweis {2 if better_hint_answer_agnostic == 0 else 1}.")
```

### 4ï¸âƒ£ Listweises Hinweisranking

Mehrere Hinweise auf einmal bewerten:

```python
print("\nAntwortbewusst gerankte Hinweise:")
ranked_hints_answer_aware = ranker.listwise_compare(question, hints, answer)
for i, (hint, _) in enumerate(ranked_hints_answer_aware):
    print(f"Rang {i + 1}: {hint}")

print("\nAntwortagnostisch gerankte Hinweise:")
ranked_hints_answer_agnostic = ranker.listwise_compare(question, hints)
for i, (hint, _) in enumerate(ranked_hints_answer_agnostic):
    print(f"Rang {i + 1}: {hint}")
```

### ğŸ“Œ Erwartete Ausgabe

```
Paarweiser Hinweisvergleich
	Antwortbewusst: Hinweis 3 ist besser als Hinweis 2.
	Antwortagnostisch: Hinweis 2 ist besser als Hinweis 1.

Listweises Hinweisranking
	Antwortbewusst gerankte Hinweise:
		Rang 1: Ã–sterreichs grÃ¶ÃŸte Stadt.
		Rang 2: Mozart und Beethoven haben hier gelebt.
		Rang 3: Es ist eine groÃŸe Stadt in Europa.

	Antwortagnostisch gerankte Hinweise:
		Rang 1: Es ist eine groÃŸe Stadt in Europa.
		Rang 2: Ã–sterreichs grÃ¶ÃŸte Stadt.
		Rang 3: Mozart und Beethoven haben hier gelebt.
```

---

## ğŸ“Š ğŸ†š Vergleich zwischen WikiHint und TriviaHG-Dataset

Die folgende Tabelle vergleicht **WikiHint** mit **TriviaHG**, dem bisher grÃ¶ÃŸten Datensatz zur Hinweisgenerierung. WikiHint zeigt eine **bessere Konvergenz**, **kÃ¼rzere Hinweise** und **hÃ¶here QualitÃ¤t**, basierend auf mehreren Evaluierungskriterien.

| **Datensatz** | **Subset** | **Relevanz** | **Lesbarkeit** | **Konvergenz** | **Vertrautheit** | **LÃ¤nge** | **Antwort-Leckage (Ã˜)** | **Antwort-Leckage (Max.)** |
|--------------|-----------|--------------|----------------|----------------|----------------|----------|----------------|----------------|
| TriviaHG     | Gesamter Datensatz | 0.95 | 0.71 | 0.57 | 0.77 | 20.82 | 0.23 | 0.44 |
| WikiHint     | Gesamter Datensatz | 0.98 | 0.72 | 0.73 | 0.75 | 17.82 | 0.24 | 0.49 |
| TriviaHG     | Training | 0.95 | 0.73 | 0.57 | 0.75 | 21.19 | 0.22 | 0.44 |
| WikiHint     | Training | 0.98 | 0.71 | 0.74 | 0.76 | 17.77 | 0.24 | 0.49 |
| TriviaHG     | Test | 0.95 | 0.73 | 0.60 | 0.77 | 20.97 | 0.23 | 0.44 |
| WikiHint     | Test | 0.98 | 0.83 | 0.72 | 0.73 | 18.32 | 0.24 | 0.47 |

ğŸ“Œ **Wichtige Erkenntnisse**:
- **WikiHint Ã¼bertrifft TriviaHG** in der **Konvergenz**, was bedeutet, dass seine Hinweise Benutzern **effektiver helfen**, zur richtigen Antwort zu gelangen.
- **WikiHint enthÃ¤lt kÃ¼rzere Hinweise**, die eine **prÃ¤zisere und effektivere UnterstÃ¼tzung** bieten.

---

## ğŸ“ŠğŸ¤– Bewertung der generierten Hinweise

Die folgende Tabelle zeigt die **Bewertung der generierten Hinweise** verschiedener **LLMs (LLaMA-3.1, GPT-4)** anhand von **Relevanz, Lesbarkeit, Konvergenz, Vertrautheit, HinweislÃ¤nge und Antwort-Leckage**. Sie gibt Einblick, wie sich **Feinabstimmung (FT)** und **Antwortbewusstsein (wA)** auf die QualitÃ¤t der Hinweise auswirken.

| **Modell** | **Konfiguration** | **Verwendet Antwort?** | **Relevanz** | **Lesbarkeit** | **Konvergenz (LLaMA-8B)** | **Konvergenz (LLaMA-70B)** | **Vertrautheit** | **LÃ¤nge** | **Antwort-Leckage (Ã˜)** | **Antwort-Leckage (Max.)** |
|-----------|----------------|------------------|------------|-------------|------------------|------------------|--------------|---------|----------------|----------------|
| **GPT-4**    | Vanilla  | âœ…      | 0.91         | 1.00           | 0.14             | 0.48             | 0.84         | 26.36   | 0.23           | 0.51           |
| **GPT-4**    | Vanilla  | âŒ      | 0.92         | 1.10           | 0.12             | 0.47             | 0.81         | 26.93   | 0.24           | 0.52           |
| **LLaMA-3.1-405b** | Vanilla  | âœ… | 0.94         | 1.49           | 0.11             | 0.47             | 0.76         | 41.81   | 0.23           | 0.50           |
| **LLaMA-3.1-405b** | Vanilla  | âŒ| 0.92         | 1.53           | 0.10             | 0.45             | 0.78         | 50.91   | 0.23           | 0.50           |
| **LLaMA-3.1-70b**  | FTwA    | âœ… | 0.88         | 1.50           | 0.09             | 0.42             | 0.84         | 43.69   | 0.22           | 0.48           |
| **LLaMA-3.1-70b**  | Vanilla  | âœ… | 0.86         | 1.53           | 0.05             | 0.42             | 0.80         | 45.51   | 0.23           | 0.50           |
| **LLaMA-3.1-70b**  | FTwoA   | âŒ | 0.86         | 1.50           | 0.08             | 0.38             | 0.80         | 51.07   | 0.22           | 0.51           |
| **LLaMA-3.1-70b**  | Vanilla  | âŒ | 0.87         | 1.56           | 0.06             | 0.38             | 0.76         | 53.24   | 0.22           | 0.50           |
| **LLaMA-3.1-8b**   | FTwA    | âœ… | 0.78         | 1.63           | 0.05             | 0.37             | 0.79         | 50.33   | 0.22           | 0.52           |
| **LLaMA-3.1-8b**   | Vanilla  | âœ… | 0.81         | 1.72           | 0.05             | 0.32             | 0.80         | 54.38   | 0.22           | 0.50           |
| **LLaMA-3.1-8b**   | FTwoA   | âŒ | 0.76         | 1.70           | 0.03             | 0.32             | 0.80         | 55.02   | 0.22           | 0.51           |
| **LLaMA-3.1-8b**   | Vanilla  | âŒ | 0.78         | 1.76           | 0.04             | 0.30             | 0.83         | 52.99   | 0.22           | 0.50           |

ğŸ“Œ **Wichtige Erkenntnisse**:
- **Relevanz**: **GrÃ¶ÃŸere Modelle (405b, 70b) generieren bessere Hinweise** als kleinere (8b).
- **Lesbarkeit**: **GPT-4 erzeugt die am besten lesbaren Hinweise**.
- **Konvergenz**: **Antwortbewusste Hinweise (wA) verbessern die QualitÃ¤t der Hinweise**.
- **Vertrautheit**: GrÃ¶ÃŸere Modelle erstellen **vertrautere Hinweise**, basierend auf allgemeinem Wissen.
- **HinweislÃ¤nge**: **Feinabgestimmte Modelle (FTwA, FTwoA) erzeugen kÃ¼rzere und bessere Hinweise**.

---

## ğŸ“‚ Verzeichnisstruktur

```
ğŸ“‚ WikiHint/                                                # ğŸ—‚ Datensatzdateien
â”‚   â”œâ”€â”€ ğŸ“„ Pipeline.png                                     # ğŸ“Š Visualisierung der Datensatz-Pipeline
â”‚   â”œâ”€â”€ ğŸ“„ training.json                                    # ğŸ“Š Trainingsdatensatz (900 Fragen, 4500 Hinweise)
â”‚   â”œâ”€â”€ ğŸ“„ test.json                                        # ğŸ“Š Testdatensatz (100 Fragen, 500 Hinweise)
â”‚
â”œâ”€â”€ ğŸ“‚ Experiments/                                         # ğŸ§ª Modellgenerierte Hinweise
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-agnostic.json
â”‚   â”œâ”€â”€ ğŸ“„ LLaMA-3.1-70b-FTwA-answer-aware.json
â”‚
â”œâ”€â”€ ğŸ“‚ HintRank/                                            # ğŸ† Methode zur Hinweisbewertung
â”‚   â”œâ”€â”€ ğŸ“„ EvaluationMethod.png                             # ğŸ“Š Visualisierung der HintRank-Methode
â”‚   â”œâ”€â”€ ğŸ“„ hint_rank.py                                     # ğŸ“œ HintRank-Implementierung
â”‚
â”œâ”€â”€ ğŸ“‚ HumanEvaluation/                                     # ğŸ‘¨â€ğŸ”¬ Menschliche Evaluationsdaten
â”‚   â”œâ”€â”€ ğŸ“‚ Person_1/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_1.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_2.xlsx
â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ Person_2/ (Gleiche Struktur wie Person_1)
â”‚   â”œâ”€â”€ ğŸ“‚ Person_3/ (Gleiche Struktur wie Person_1)
â”‚
â””â”€â”€ ğŸ“˜ README.md                                            # ğŸ“– Diese Datei
```

---

## ğŸ“œ Lizenz

Dieses Projekt steht unter der **Creative Commons Attribution 4.0 International License (CC BY 4.0)**. Sie kÃ¶nnen den Datensatz frei nutzen, teilen und anpassen, solange eine korrekte Quellenangabe erfolgt.

---

## ğŸ“‘ Zitation

Falls Sie unsere Arbeit nÃ¼tzlich finden, zitieren Sie bitte [ğŸ“œ unser Paper](https://doi.org/10.48550/arXiv.2412.01626):

Mozafari, J., Gerhold, F., & Jatowt, A. (2024). WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation. arXiv preprint arXiv:2412.01626.

### ğŸ“„ BibTeX:
```bibtex
@article{mozafari2025wikihinthumanannotateddatasethint,
      title={WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation}, 
      author={Jamshid Mozafari and Florian Gerhold and Adam Jatowt},
      year={2025},
      eprint={2412.01626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2412.01626}, 
}
```

---

## ğŸ™ Danksagungen

Vielen Dank an unsere Mitwirkenden und die UniversitÃ¤t Innsbruck fÃ¼r die UnterstÃ¼tzung dieses Projekts. ğŸš€
