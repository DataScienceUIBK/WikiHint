# WikiHint: Ein von Menschen annotierter Datensatz fÃ¼r Hinweisbewertung und -generierung

<a href="https://doi.org/10.48550/arXiv.2412.01626"><img src="https://img.shields.io/static/v1?label=Paper&message=arXiv&color=green&logo=arxiv"></a>
<a href="https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb"><img src="https://img.shields.io/static/v1?label=Colab&message=Demo&logo=Google%20Colab&color=f9ab00"></a>
[![License](https://img.shields.io/badge/License-CC%20BY%204.0-blue)](https://creativecommons.org/licenses/by/4.0/)
[![en](https://img.shields.io/badge/Lang-En-red.svg)](https://github.com/DataScienceUIBK/WikiHint/blob/main/README.md)
[![ch](https://img.shields.io/badge/Lang-Ch-brown.svg)](https://github.com/DataScienceUIBK/WikiHint/blob/main/README.ch.md)
<img src="https://github.com/DataScienceUIBK/WikiHint/blob/main/WikiHint/Pipeline.png">

WikiHint ist ein **von Menschen annotierter Datensatz**, der fÃ¼r die **automatische Generierung und Bewertung von Hinweisen** bei Faktfragen entwickelt wurde. Der Datensatz basiert auf Wikipedia und enthÃ¤lt **5.000 Hinweise zu 1.000 Fragen**. Er unterstÃ¼tzt die Forschung in den Bereichen **Hinweisbewertung, -ranking und -generierung**.

## ğŸ—‚ Ãœberblick

- **1.000 Fragen** mit **5.000 manuell erstellten Hinweisen**.
- Hinweise werden von **menschlichen Annotatoren** nach NÃ¼tzlichkeit bewertet.
- Evaluierung durch **LLMs (LLaMA, GPT-4)** und **Studien zur menschlichen Leistung**.
- UnterstÃ¼tzt **Hinweisranking** und **automatische Hinweisbewertung**.

## ğŸ”¬ Wissenschaftliche BeitrÃ¤ge

âœ… **Erster von Menschen annotierter Datensatz** fÃ¼r die Generierung und Bewertung von Hinweisen.  
âœ… **HintRank:** Eine leichtgewichtige Methode zur automatischen Hinweisbewertung.  
âœ… **Human-Studie** zur Bewertung der EffektivitÃ¤t von Hinweisen fÃ¼r Benutzer.  
âœ… **Feinabstimmung von Open-Source-LLMs** (LLaMA-3.1, GPT-4) zur Hinweisgenerierung.  

## ğŸ“ˆ Zentrale Erkenntnisse

- **Antwortbewusste Hinweise** verbessern die EffektivitÃ¤t.  
- **Feinabgestimmte LLaMA-Modelle** erzeugen bessere Hinweise als Standardmodelle.  
- **KÃ¼rzere Hinweise** sind tendenziell **effektiver** als lÃ¤ngere.  
- **Von Menschen erstellte Hinweise** Ã¼bertreffen LLM-generierte Hinweise in Klarheit und Ranking.

## ğŸš€ Erste Schritte

### 1ï¸âƒ£ Repository klonen

```sh
git clone https://github.com/DataScienceUIBK/WikiHint.git
cd WikiHint
```

### 2ï¸âƒ£ Datensatz laden

```python
import json

with open("./WikiHint/training.json", "r") as f:
    training_data = json.load(f)

with open("./WikiHint/test.json", "r") as f:
    test_data = json.load(f)

print(f"Trainingsset: {len(training_data)} Fragen")
print(f"Testset: {len(test_data)} Fragen")
```

## ğŸ† HintRank: Eine leichtgewichtige Methode zur Hinweisbewertung

HintRank ist eine **automatische Methode zur Bewertung von Hinweisen**, die auf **BERT-basierten Modellen** beruht. Sie nutzt **paarweise Vergleiche**, um die **relative NÃ¼tzlichkeit von Hinweisen** zu bestimmen.

<p align="center">
    <img src="https://raw.githubusercontent.com/DataScienceUIBK/WikiHint/main/HintRank/EvaluationMethod.png" width="35%">
</p>

### âœ¨ Funktionen:
âœ” **Leichtgewichtig**: LÃ¤uft lokal ohne hohen Rechenaufwand.  
âœ” **LLM-freie Bewertung**: Funktioniert ohne **groÃŸe generative Modelle**.  
âœ” **An Menschen ausgerichtetes Ranking**: Hohe Korrelation mit **von Menschen zugewiesenen Hinweisbewertungen**.  

### ğŸ” Funktionsweise:
1. **VerknÃ¼pft Frage & zwei Hinweise** â†’ Konvertiert sie in ein BERT-kompatibles Format.  
2. **Berechnet die QualitÃ¤t der Hinweise** â†’ Bestimmt, welcher Hinweis **nÃ¼tzlicher** ist.  
3. **Erstellt ein Hinweisranking** â†’ Vergibt Platzierungen basierend auf paarweisen Vergleichen.  

## ğŸ“Œ Nutzung von `HintRank` zur Hinweisbewertung

Das `HintRank`-Modul dient zur **automatischen Bewertung von Hinweisen** anhand ihrer NÃ¼tzlichkeit mit **BERT-basierten Modellen**.

### ğŸš€ HintRank-Demo in Google Colab ausfÃ¼hren

Du kannst **HintRank** einfach in deinem Browser Ã¼ber **Google Colab** ausprobieren, ohne eine lokale Installation vornehmen zu mÃ¼ssen. Starte einfach das **[Colab-Notebook](https://colab.research.google.com/github/DataScienceUIBK/WikiHint/blob/main/HintRank/Demo.ipynb)** und erkunde **HintRank** interaktiv.

### 1ï¸âƒ£ AbhÃ¤ngigkeiten installieren

Falls du `HintRank` lokal ausfÃ¼hrst, installiere die erforderlichen AbhÃ¤ngigkeiten:

```sh
pip install transformers torch numpy scipy
```

### 2ï¸âƒ£ HintRank importieren und initialisieren

Navigiere in das `HintRank`-Verzeichnis und importiere das `hint_rank`-Modul:

```python
from HintRank.hint_rank import HintRank

# HintRank-Modell initialisieren
ranker = HintRank()
```

### 3ï¸âƒ£ Hinweise fÃ¼r eine Frage bewerten

```python
question = "Was ist die Hauptstadt von Ã–sterreich?"
answer = "Wien"
hints = [
    "Mozart und Beethoven haben hier gelebt.",
    "Es ist eine groÃŸe Stadt in Europa.",
    "Ã–sterreichs grÃ¶ÃŸte Stadt."
]

# Beispiel fÃ¼r paarweisen Vergleich
better_hint_answer_aware = ranker.pairwise_compare(question, hints[1], hints[2], answer)
better_hint_answer_agnostic = ranker.pairwise_compare(question, hints[0], hints[1])

print(f"Antwortbewusst: Hinweis {2 if better_hint_answer_aware == 1 else 3} ist besser als Hinweis {3 if better_hint_answer_aware == 0 else 2}.")
print(f"Antwortagnostisch: Hinweis {1 if better_hint_answer_agnostic == 1 else 2} ist besser als Hinweis {2 if better_hint_answer_agnostic == 0 else 1}.")
```

### 4ï¸âƒ£ Listweises Hinweisranking

Mehrere Hinweise auf einmal bewerten:

```python
print("\nAntwortbewusst gerankte Hinweise:")
ranked_hints_answer_aware = ranker.listwise_compare(question, hints, answer)
for i, (hint, _) in enumerate(ranked_hints_answer_aware):
    print(f"Rang {i + 1}: {hint}")

print("\nAntwortagnostisch gerankte Hinweise:")
ranked_hints_answer_agnostic = ranker.listwise_compare(question, hints)
for i, (hint, _) in enumerate(ranked_hints_answer_agnostic):
    print(f"Rang {i + 1}: {hint}")
```

---

### ğŸ“Œ Erwartete Ausgabe

```
Paarweiser Hinweisvergleich
	Antwortbewusst: Hinweis 3 ist besser als Hinweis 2.
	Antwortagnostisch: Hinweis 2 ist besser als Hinweis 1.

Listweises Hinweisranking
	Antwortbewusst gerankte Hinweise:
		Rang 1: Ã–sterreichs grÃ¶ÃŸte Stadt.
		Rang 2: Mozart und Beethoven haben hier gelebt.
		Rang 3: Es ist eine groÃŸe Stadt in Europa.

	Antwortagnostisch gerankte Hinweise:
		Rang 1: Es ist eine groÃŸe Stadt in Europa.
		Rang 2: Ã–sterreichs grÃ¶ÃŸte Stadt.
		Rang 3: Mozart und Beethoven haben hier gelebt.
```

---

## ğŸ“Š ğŸ†š Vergleich zwischen WikiHint und TriviaHG-Dataset

Die folgende Tabelle vergleicht **WikiHint** mit **TriviaHG**, dem bisher grÃ¶ÃŸten Datensatz zur Hinweisgenerierung. WikiHint zeigt eine **bessere Konvergenz**, **kÃ¼rzere Hinweise** und **hÃ¶here QualitÃ¤t**, basierend auf mehreren Evaluierungskriterien.

| **Datensatz** | **Subset** | **Relevanz** | **Lesbarkeit** | **Konvergenz** | **Vertrautheit** | **LÃ¤nge** | **Antwort-Leckage (Ã˜)** | **Antwort-Leckage (Max.)** |
|--------------|-----------|--------------|----------------|----------------|----------------|----------|----------------|----------------|
| TriviaHG     | Gesamter Datensatz | 0.95 | 0.71 | 0.57 | 0.77 | 20.82 | 0.23 | 0.44 |
| WikiHint     | Gesamter Datensatz | 0.98 | 0.72 | 0.73 | 0.75 | 17.82 | 0.24 | 0.49 |
| TriviaHG     | Training | 0.95 | 0.73 | 0.57 | 0.75 | 21.19 | 0.22 | 0.44 |
| WikiHint     | Training | 0.98 | 0.71 | 0.74 | 0.76 | 17.77 | 0.24 | 0.49 |
| TriviaHG     | Test | 0.95 | 0.73 | 0.60 | 0.77 | 20.97 | 0.23 | 0.44 |
| WikiHint     | Test | 0.98 | 0.83 | 0.72 | 0.73 | 18.32 | 0.24 | 0.47 |

ğŸ“Œ **Wichtige Erkenntnisse**:
- **WikiHint Ã¼bertrifft TriviaHG** in der **Konvergenz**, was bedeutet, dass seine Hinweise Benutzern **effektiver helfen**, zur richtigen Antwort zu gelangen.
- **WikiHint enthÃ¤lt kÃ¼rzere Hinweise**, die eine **prÃ¤zisere und effektivere UnterstÃ¼tzung** bieten.

---

## ğŸ“ŠğŸ¤– Bewertung der generierten Hinweise

Die folgende Tabelle zeigt die **Bewertung der generierten Hinweise** verschiedener **LLMs (LLaMA-3.1, GPT-4)** anhand von **Relevanz, Lesbarkeit, Konvergenz, Vertrautheit, HinweislÃ¤nge und Antwort-Leckage**. Sie gibt Einblick, wie sich **Feinabstimmung (FT)** und **Antwortbewusstsein (wA)** auf die QualitÃ¤t der Hinweise auswirken.

| **Modell** | **Konfiguration** | **Verwendet Antwort?** | **Relevanz** | **Lesbarkeit** | **Konvergenz (LLaMA-8B)** | **Konvergenz (LLaMA-70B)** | **Vertrautheit** | **LÃ¤nge** | **Antwort-Leckage (Ã˜)** | **Antwort-Leckage (Max.)** |
|-----------|----------------|------------------|------------|-------------|------------------|------------------|--------------|---------|----------------|----------------|
| **GPT-4** | Vanilla | âœ… | 0.91 | 1.00 | 0.14 | 0.48 | 0.84 | 26.36 | 0.23 | 0.51 |
| **GPT-4** | Vanilla | âŒ | 0.92 | 1.10 | 0.12 | 0.47 | 0.81 | 26.93 | 0.24 | 0.52 |
| **LLaMA-3.1-405b** | Vanilla | âœ… | 0.94 | 1.49 | 0.11 | 0.47 | 0.76 | 41.81 | 0.23 | 0.50 |
| **LLaMA-3.1-70b** | FTwA | âœ… | 0.88 | 1.50 | 0.09 | 0.42 | 0.84 | 43.69 | 0.22 | 0.48 |
| **LLaMA-3.1-70b** | FTwoA | âŒ | 0.86 | 1.50 | 0.08 | 0.38 | 0.80 | 51.07 | 0.22 | 0.51 |

ğŸ“Œ **Wichtige Erkenntnisse**:
- **Relevanz**: **GrÃ¶ÃŸere Modelle (405b, 70b) generieren bessere Hinweise** als kleinere (8b).
- **Lesbarkeit**: **GPT-4 erzeugt die am besten lesbaren Hinweise**.
- **Konvergenz**: **Antwortbewusste Hinweise (wA) verbessern die QualitÃ¤t der Hinweise**.
- **Vertrautheit**: GrÃ¶ÃŸere Modelle erstellen **vertrautere Hinweise**, basierend auf allgemeinem Wissen.
- **HinweislÃ¤nge**: **Feinabgestimmte Modelle (FTwA, FTwoA) erzeugen kÃ¼rzere und bessere Hinweise**.

---

## ğŸ“‚ Verzeichnisstruktur

```
ğŸ“‚ WikiHint/                                                # ğŸ—‚ Datensatzdateien
â”‚   â”œâ”€â”€ ğŸ“„ Pipeline.png                                     # ğŸ“Š Visualisierung der Datensatz-Pipeline
â”‚   â”œâ”€â”€ ğŸ“„ training.json                                    # ğŸ“Š Trainingsdatensatz (900 Fragen, 4500 Hinweise)
â”‚   â”œâ”€â”€ ğŸ“„ test.json                                        # ğŸ“Š Testdatensatz (100 Fragen, 500 Hinweise)
â”‚
â”œâ”€â”€ ğŸ“‚ Experiments/                                         # ğŸ§ª Modellgenerierte Hinweise
â”‚   â”œâ”€â”€ ğŸ“„ GPT-4-Vanilla-answer-agnostic.json
â”‚   â”œâ”€â”€ ğŸ“„ LLaMA-3.1-70b-FTwA-answer-aware.json
â”‚
â”œâ”€â”€ ğŸ“‚ HintRank/                                            # ğŸ† Methode zur Hinweisbewertung
â”‚   â”œâ”€â”€ ğŸ“„ EvaluationMethod.png                             # ğŸ“Š Visualisierung der HintRank-Methode
â”‚   â”œâ”€â”€ ğŸ“„ hint_rank.py                                     # ğŸ“œ HintRank-Implementierung
â”‚
â”œâ”€â”€ ğŸ“‚ HumanEvaluation/                                     # ğŸ‘¨â€ğŸ”¬ Menschliche Evaluationsdaten
â”‚   â”œâ”€â”€ ğŸ“‚ Person_1/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_1.xlsx
â”‚   â”‚   â”œâ”€â”€ ğŸ“‘ Part_2.xlsx
â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ Person_2/ (Gleiche Struktur wie Person_1)
â”‚   â”œâ”€â”€ ğŸ“‚ Person_3/ (Gleiche Struktur wie Person_1)
â”‚
â””â”€â”€ ğŸ“˜ README.md                                            # ğŸ“– Diese Datei
```

---

## ğŸ“œ Lizenz

Dieses Projekt steht unter der **Creative Commons Attribution 4.0 International License (CC BY 4.0)**. Sie kÃ¶nnen den Datensatz frei nutzen, teilen und anpassen, solange eine korrekte Quellenangabe erfolgt.

---

## ğŸ“‘ Zitation

Falls Sie unsere Arbeit nÃ¼tzlich finden, zitieren Sie bitte [ğŸ“œ unser Paper](https://doi.org/10.48550/arXiv.2412.01626):

Mozafari, J., Gerhold, F., & Jatowt, A. (2024). WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation. arXiv preprint arXiv:2412.01626.

### ğŸ“„ BibTeX:
```bibtex
@article{mozafari2025wikihinthumanannotateddatasethint,
      title={WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation}, 
      author={Jamshid Mozafari and Florian Gerhold and Adam Jatowt},
      year={2025},
      eprint={2412.01626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2412.01626}, 
}
```

---

## ğŸ™ Danksagungen

Vielen Dank an unsere Mitwirkenden und die UniversitÃ¤t Innsbruck fÃ¼r die UnterstÃ¼tzung dieses Projekts. ğŸš€
